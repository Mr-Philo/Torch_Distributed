{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch的自动微分算法 auto_grad\n",
    "\n",
    "在深入了解Pytorch自动微分算法前，需要对pytorch的自动微分算法有一定了解，这样在涉及到反向传播时梯度在多卡之间的通信时才能更加熟练\n",
    "\n",
    "反向传播的原理在此不再赘述，我们只关注反向传播和自动微分在Pytorch中的实现，且更多关注API。首先导入torch，定义示例向量。注意这里我们指定向量是需要梯度的，即`require_grad=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_1 = torch.tensor(2.0, requires_grad=True)\n",
    "b_1 = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义一个函数:\n",
    "\n",
    "$$ y_1 = x_1^3 + b_1 $$\n",
    "\n",
    "显然，$y_1$对$x_1$的导数为$\\frac{y_1}{x_1}=3x^2$，对$b_1$的导数为1。我们利用代码来验证之："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<AddBackward0>) tensor(12.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "y_1 = x_1 ** 3 + b_1\n",
    "y_1.backward()\n",
    "print(y_1, x_1.grad, b_1.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了直接对输出值$y_1$使用`.backward()`方法，还可以通过`torch.autograd.grad()`函数来进行自动求导。该函数接受输入值为待求导的方程、输入变量，返回值为梯度，**其中梯度的形状和输入变量的形状一模一样**\n",
    "\n",
    "注意：前面所使用的`.backward()`方法返回值始终为`None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(12.), tensor(1.))\n"
     ]
    }
   ],
   "source": [
    "x_2 = torch.tensor(2.0, requires_grad=True)\n",
    "b_2 = torch.tensor(1.0, requires_grad=True)\n",
    "y_2 = x_2 ** 3 + b_2\n",
    "grads = torch.autograd.grad(y_2, inputs=[x_2, b_2])\n",
    "print(grads)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于高维的输入输出，正常计算导数需要用到雅可比矩阵，而在pytorch内实现则非常方便。此时梯度的形状与自变量的形状一致。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 5.,  8.],\n",
      "        [11., 14.]]), tensor([[ 4.,  7.],\n",
      "        [10., 13.]]))\n",
      "torch.Size([2, 2]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[2.0, 3.0], [4.0, 5.0]], requires_grad=True)\n",
    "b = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)  # 2x2\n",
    "z = a ** 2 + a * b + b ** 2\n",
    "grads = torch.autograd.grad(z, inputs=[a, b], grad_outputs=torch.ones_like(z))\n",
    "print(grads)\n",
    "print(grads[0].shape, grads[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们引用pytorch官方tutorial的一个简单的神经网络的例子。考虑最简单的一层神经网络，具有输入`x`、参数`w`和`b`，以及一些损失函数。它可以通过以下方式在 PyTorch 中定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)               # input tensor\n",
    "y = torch.zeros(3)              # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中`w`和`b`是我们想要优化的参数，因此我们设置了他们的属性`requires_grad=True`。除此之外，还可以在稍后使用`w.requires_grad_(True)`来设置。\n",
    "\n",
    "众所周知，神经网络可以看作一张计算图。我们将上述这些矢量构建到计算图内，实际上是创建了一个`Function`类的对象，该对象知道如何在前向计算函数，以及如何在反向传播步骤中计算其导数。对反向传播函数的引用存储在`grad_fn`张量的属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f53d04e7e50>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f53d04e7f40>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络的训练过程中，我们希望能从网络的前向输出计算梯度，从而对参数`w`和`b`进行更新。我们采用最简单的`.backward()`方法，计算并观测这些参数的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0255, 0.0777, 0.0249],\n",
      "        [0.0255, 0.0777, 0.0249],\n",
      "        [0.0255, 0.0777, 0.0249],\n",
      "        [0.0255, 0.0777, 0.0249],\n",
      "        [0.0255, 0.0777, 0.0249]])\n",
      "tensor([0.0255, 0.0777, 0.0249])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有两点需要注意：\n",
    "\n",
    "1. 只有设置为`requires_grad=True`的参数（或言：计算图中的叶子节点）才能得到它们的`grad`属性，其他的节点没法得到梯度；\n",
    "\n",
    "2. 正常情况下，为了优化性能，一次前向、后向计算完毕后，该计算图就会被销毁。如果我们需要在同一个图上多次调用backward，则需要在backward调用时候设置 `retain_graph=True`。\n",
    "\n",
    "在神经网络的验证valid/测试test过程中，为了加快进度，我们不需要也不必计算梯度，此时我们可以利用pytorch提供的`torch.no_grad()`函数来实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用了`with torch.no_grad():`环境后，所有梯度计算都会禁止梯度的计算。实现相同结果的另一种方法是在张量上使用`detach()`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大神经网络的例子\n",
    "\n",
    "一般来说常用的代码库都会将神经网络利用`torch.nn.Module`来进行封装。接下来演示在这种更常见的情况下，如何访问和操作模型参数的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 8]) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 一个简单的MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(8, 12)  # Hidden layer\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(12, 4)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)\n",
    " \n",
    "# 实例化MLP类得到net\n",
    "net = MLP()\n",
    "\n",
    "# 访问net的第一个隐藏层的权重和偏差\n",
    "w1 = net.hidden.weight\n",
    "b1 = net.hidden.bias\n",
    "print(w1.shape, b1.shape) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里采用的是`net.<name>.weight`的方式来访问权重的。如果不清楚模型中每一层的名字，可以采用以下方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden.weight torch.Size([12, 8])\n",
      "hidden.bias torch.Size([12])\n",
      "output.weight torch.Size([4, 12])\n",
      "output.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此基础上，访问这些权重的梯度只需要多索引一次`.grad`即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "# 访问net的第一个隐藏层的权重和偏差的梯度\n",
    "w1_grad = net.hidden.weight.grad \n",
    "b1_grad = net.hidden.bias.grad\n",
    "print(w1_grad, b1_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里的输出虽然为`None`，但这些层的`require_grad`在`nn.Module`内是默认为`True`的，输出为空只是因为网络刚被初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(net.hidden.weight.requires_grad)\n",
    "print(net.hidden.bias.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假装对该网络做一个训练，可以看到网络的参数的梯度被计算，进而被应用来更新参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.7470499873161316, weight grad for hidden layer: tensor([[ 1.5022e-02, -3.0880e-03,  3.0836e-03, -1.2889e-02, -3.2447e-02,\n",
      "          4.8631e-03,  7.8574e-03, -4.6769e-03],\n",
      "        [ 1.2990e-02, -9.3525e-03, -1.7760e-02,  4.2679e-03, -1.1164e-02,\n",
      "         -2.3242e-03, -5.0472e-03,  2.7768e-03],\n",
      "        [-1.3842e-02,  1.3202e-02,  1.6635e-02, -9.7227e-03,  3.8987e-03,\n",
      "         -6.7535e-03,  3.0358e-03,  3.1507e-03],\n",
      "        [ 1.3816e-02, -7.6427e-03, -1.2734e-03,  5.0467e-04, -7.3625e-03,\n",
      "          2.8755e-03, -4.0851e-03,  3.6573e-03],\n",
      "        [-2.5245e-02,  1.2688e-02, -2.0006e-02,  8.2256e-02,  3.5192e-02,\n",
      "          1.9001e-03, -9.4666e-04, -3.8687e-02],\n",
      "        [-3.8535e-03,  5.7771e-03, -1.2471e-02,  3.4457e-02,  1.4222e-02,\n",
      "          5.2874e-03, -2.0068e-03, -2.3625e-02],\n",
      "        [-2.4186e-02,  3.9408e-03, -1.4809e-02, -1.6549e-03,  6.2134e-03,\n",
      "         -1.1559e-02, -6.6432e-03, -5.0576e-04],\n",
      "        [-3.1156e-02,  2.5198e-02, -2.7281e-03,  6.1452e-02,  5.2364e-02,\n",
      "         -1.1949e-02, -1.8314e-02, -1.3493e-02],\n",
      "        [-2.6992e-02,  1.8017e-02,  5.6233e-02, -8.0016e-02, -4.7865e-02,\n",
      "         -2.1997e-02, -1.3876e-02, -1.8143e-02],\n",
      "        [ 2.9495e-03,  8.1803e-04, -8.1870e-04,  3.3467e-04, -6.4921e-04,\n",
      "         -4.5443e-04, -7.3260e-04, -6.0855e-04],\n",
      "        [-3.4647e-02,  8.5033e-03,  5.8557e-03, -4.0388e-04,  3.1920e-02,\n",
      "         -1.0868e-02, -6.0448e-03, -5.6404e-03],\n",
      "        [ 8.6887e-05,  1.4615e-02,  1.6988e-03,  2.7568e-03,  2.6336e-03,\n",
      "         -6.8317e-04, -5.0253e-03, -5.6342e-03]], device='cuda:0')\n",
      "step: 1, loss: 0.7557422518730164, weight grad for hidden layer: tensor([[ 0.0180, -0.0044,  0.0027, -0.0138, -0.0338,  0.0042,  0.0045, -0.0008],\n",
      "        [-0.0089,  0.0046, -0.0455,  0.0027,  0.0156,  0.0051, -0.0077, -0.0247],\n",
      "        [-0.0119,  0.0238,  0.0396, -0.0065, -0.0380,  0.0133,  0.0282,  0.0111],\n",
      "        [-0.0079, -0.0216, -0.0232,  0.0143, -0.0186,  0.0409,  0.0357,  0.0257],\n",
      "        [-0.0493,  0.0114, -0.0310,  0.0887,  0.0367,  0.0277,  0.0259, -0.0321],\n",
      "        [-0.0060, -0.0008, -0.0138,  0.0309,  0.0215,  0.0074, -0.0041, -0.0201],\n",
      "        [-0.0259, -0.0056, -0.0216, -0.0010,  0.0055, -0.0077, -0.0089,  0.0071],\n",
      "        [-0.0508,  0.0322, -0.0301,  0.0671,  0.0504, -0.0089, -0.0153, -0.0401],\n",
      "        [ 0.0134,  0.0117,  0.0756, -0.0840, -0.0428, -0.0483, -0.0334, -0.0079],\n",
      "        [ 0.0107, -0.0162,  0.0007, -0.0024,  0.0049, -0.0158, -0.0277,  0.0313],\n",
      "        [-0.0322, -0.0077, -0.0103,  0.0081,  0.0329, -0.0222, -0.0125,  0.0046],\n",
      "        [ 0.0008, -0.0013,  0.0076, -0.0065,  0.0031, -0.0091, -0.0258,  0.0151]],\n",
      "       device='cuda:0')\n",
      "step: 2, loss: 0.9090520739555359, weight grad for hidden layer: tensor([[ 2.5772e-02,  2.3907e-02,  7.9763e-03, -2.7942e-02, -5.3790e-02,\n",
      "          8.9143e-03,  2.3669e-02,  1.0116e-02],\n",
      "        [ 1.0080e-02, -1.1371e-02, -8.2905e-02, -1.9889e-02,  8.9306e-03,\n",
      "         -1.1296e-02, -2.0478e-03, -3.9222e-02],\n",
      "        [-6.6314e-02,  2.7263e-02,  2.5666e-02, -1.0403e-02, -2.6554e-02,\n",
      "          1.6981e-02,  3.1534e-03,  3.0396e-03],\n",
      "        [ 5.0575e-02, -8.1270e-02, -8.5559e-02, -2.3315e-02, -3.2585e-02,\n",
      "         -1.3412e-02,  6.2104e-02,  3.3442e-03],\n",
      "        [ 1.6934e-02, -8.3853e-02,  1.8102e-02,  1.6690e-01,  4.4249e-02,\n",
      "         -1.0332e-02, -9.7204e-03, -2.4073e-02],\n",
      "        [ 3.0816e-03, -1.3725e-02,  1.6857e-02,  6.1053e-02,  4.4163e-03,\n",
      "         -1.9429e-03, -3.8668e-02, -4.5429e-03],\n",
      "        [-2.4686e-02, -1.9319e-03, -2.0341e-02,  6.0378e-05,  6.6187e-03,\n",
      "         -1.1869e-02, -5.9042e-03,  1.1448e-02],\n",
      "        [-4.6724e-02,  3.1188e-03, -6.9950e-02,  6.9525e-02,  3.5695e-02,\n",
      "         -5.0431e-02, -3.5229e-02, -8.2335e-02],\n",
      "        [ 1.4035e-02,  6.5895e-03,  7.9734e-02, -8.7354e-02, -5.1247e-02,\n",
      "         -4.1529e-02, -4.0718e-02, -1.9310e-02],\n",
      "        [ 1.0729e-02, -1.6215e-02,  6.7164e-04, -2.3677e-03,  4.9494e-03,\n",
      "         -1.5791e-02, -2.7715e-02,  3.1294e-02],\n",
      "        [-3.0407e-02, -9.1384e-02, -5.3900e-02,  2.5145e-02,  4.1887e-02,\n",
      "         -6.3934e-02, -5.0409e-02, -4.4053e-02],\n",
      "        [ 4.3970e-03,  3.8908e-03,  7.0205e-03, -6.8260e-03,  2.8890e-03,\n",
      "         -1.9741e-02, -2.1834e-02,  1.8651e-02]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 做一个假的数据集，30个样本，每个样本8维特征，4维标签\n",
    "data = torch.randn(30, 8)\n",
    "label = torch.randn(30, 4)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data, label)\n",
    "dataloder = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# 开始训练\n",
    "net.train()\n",
    "net.cuda()      # 可选，如果有GPU的话\n",
    "\n",
    "# 我们只训练一个epoch即可\n",
    "for index, (data, label) in enumerate(dataloder):\n",
    "    data = data.cuda()\n",
    "    label = label.cuda()\n",
    "    out = net(data)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(out, label)\n",
    "    loss.backward()\n",
    "    print(f\"step: {index}, loss: {loss}, weight grad for hidden layer: {net.hidden.weight.grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见在训练阶段，模型的参数的梯度被计算，进而用于更新模型参数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
