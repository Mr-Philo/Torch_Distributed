{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeed API\n",
    "\n",
    "这一节介绍DeepSpeed所提供的API，主要关注ZeRO零冗余优化器\n",
    "\n",
    "零冗余优化器 (ZeRO) 通过在分布式代码运行中将三个模型状态（优化器状态、梯度和参数）划分到不同GPU上来消除并行进程的内存冗余。通过这样做，与单纯数据并行相比提高了内存效率，同时保留了其计算粒度和通信效率。\n",
    "\n",
    "1. **ZeRO Stage 1**：优化器状态（例如，对于 Adam 优化器、32 位权重以及一阶和二阶矩估计）在进程之间进行分区，以便每个进程仅更新其分区中的那一部分。\n",
    "\n",
    "2. **ZeRO Stage 2**：用于更新模型权重的减少的 32 位梯度也被分区，这样每个进程只保留与其优化器状态部分对应的梯度。\n",
    "\n",
    "3. **ZeRO Stage 3**：16 位模型参数跨进程分区。 ZeRO-3 将在前向和后向传递过程中自动收集和划分它们。\n",
    "\n",
    "此外，ZeRO-3 包含infinity offload engine以形成 ZeRO-Infinity（[论文](https://arxiv.org/abs/2104.07857)），它可以将所有模型状态卸载到 CPU 和 NVMe 内存，以获得巨大的内存节约。如需深入了解算法，可以参阅有关论文。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO config\n",
    "\n",
    "更改ZeRO优化器各种设置的一个重要文件便是其config文件。完整config选项可见源码deepspeed.runtime.zero.config.DeepSpeedZeroConfig文件，这里只列举比较重要的几项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroStageEnum(int, Enum):\n",
    "    \"\"\" Enum class for possible zero stages \"\"\"\n",
    "    disabled = 0\n",
    "    optimizer_states = 1\n",
    "    gradients = 2\n",
    "    weights = 3\n",
    "    max_stage = 3\n",
    "\n",
    "class DeepSpeedZeroConfig(DeepSpeedConfigModel):\n",
    "    \"\"\"\n",
    "    Sets parameters for ZeRO optimizations.\n",
    "    \"\"\"\n",
    "\n",
    "    stage: ZeroStageEnum = 0\n",
    "    \"\"\"\n",
    "    0：关闭  1：划分优化器状态  2：划分优化器状态和梯度状态  3：划分优化器状态、梯度状态和参数状态\n",
    "    \"\"\"\n",
    "\n",
    "    contiguous_gradients: bool = True\n",
    "    \"\"\"\n",
    "    将梯度复制到连续缓冲区中，以便在反向传播过程中避免内存碎片\n",
    "    \"\"\"\n",
    "\n",
    "    reduce_scatter: bool = True\n",
    "    \"\"\"\n",
    "    在梯度平均时，使用reduce或reduce scatter而不是allreduce\n",
    "    \"\"\"\n",
    "\n",
    "    offload_param: Optional[DeepSpeedZeroOffloadParamConfig] = None\n",
    "    \"\"\"\n",
    "    启用将模型参数卸载到 CPU 或 NVMe。这可以为更大的模型或批量大小释放 GPU 内存。仅对Stage 3 有效。\n",
    "    \"\"\"\n",
    "\n",
    "    offload_optimizer: Optional[DeepSpeedZeroOffloadOptimizerConfig] = None\n",
    "    \"\"\"\n",
    "    启用将优化器状态卸载到 CPU 或 NVMe。这可以为更大的模型或批量大小释放 GPU 内存。对Stage 1、2、3 有效。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    gather_16bit_weights_on_model_save: bool = Field(False, alias=\"stage3_gather_16bit_weights_on_model_save\")\n",
    "    \"\"\"\n",
    "    在通过 save_16bit_model() 保存模型之前合并权重。由于权重是跨 GPU 分区的，它们不是 state_dict 的一部分，因此此函数会在启用此选项时自动收集权重，然后保存 fp16 模型权重。\n",
    "    \"\"\"\n",
    "\n",
    "    # Validators\n",
    "    @validator(\"overlap_comm\")\n",
    "    def overlap_comm_valid(cls, field_value, values):\n",
    "        if field_value is None:\n",
    "            assert (\"stage\" in values), \"DeepSpeedZeroConfig: 'stage' must be defined before 'overlap_comm'\"\n",
    "            field_value = values[\"stage\"] == ZeroStageEnum.weights\n",
    "        return field_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于ZeRO config，还有关于Offload的两个类：\n",
    "\n",
    "`classdeepspeed.runtime.zero.config.DeepSpeedZeroOffloadParamConfig`\n",
    "\n",
    "`classdeepspeed.runtime.zero.config.DeepSpeedZeroOffloadOptimizerConfig`\n",
    "\n",
    "分别是关于参数和优化器状态的offload，这里先忽略之\n",
    "\n",
    "在config.json文件里面所撰写的ZeRO优化器设置大致包括：\n",
    "\n",
    "```json\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": [0|1|2|3],\n",
    "    \"allgather_partitions\": [true|false],\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": false,\n",
    "    \"reduce_scatter\": [true|false],\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"contiguous_gradients\" : [true|false],\n",
    "    \"offload_param\": {\n",
    "      ...\n",
    "    },\n",
    "    \"offload_optimizer\": {\n",
    "      ...\n",
    "    },\n",
    "    \"stage3_max_live_parameters\" : 1e9,\n",
    "    \"stage3_max_reuse_distance\" : 1e9,\n",
    "    \"stage3_prefetch_bucket_size\" : 5e8,\n",
    "    \"stage3_param_persistence_threshold\" : 1e6,\n",
    "    \"sub_group_size\" : 1e12,\n",
    "    \"elastic_checkpoint\" : [true|false],\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": [true|false],\n",
    "    \"ignore_unused_parameters\": [true|false]\n",
    "    \"round_robin_gradients\": [true|false]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO Debugging\n",
    "\n",
    "如果想要在debug的过程中访问模型参数、梯度和优化器状态，DeepSpeed 提供了以下API来访问他们，其中各参数都是以未被分区的形式来呈现的。\n",
    "\n",
    "**重要提示**：请注意，参加training所有进程都必须调用这些实用程序，即使我们只想在主进程中对结果执行某些操作。如果没有所有进程都参与，这些API将被挂起。\n",
    "\n",
    "此外，必须在特定的阶段访问正确的参数。例如，梯度在`backward`之后、`step`之前有效。优化器状态在`step`之后更新。 fp32 主权重也是如此。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in deepspeed/utils/tensor_fragment.py\n",
    "\n",
    "# 收集并访问各分区fp16参数，返回fp32参数\n",
    "def safe_get_full_fp32_param(param):\n",
    "    \"\"\"Assemble and return the fp32 parameter of a low-precision (e.g., fp16) parameter.\n",
    "\n",
    "        Args:\n",
    "            param (``torch.nn.Parameter``): A model parameter\n",
    "    \"\"\"\n",
    "    # ZeRO stage 3 param\n",
    "    if hasattr(param, 'ds_id'):\n",
    "        return param._z3_optimizer.get_full_hp_param(param)\n",
    "\n",
    "    # ZeRO stage 1, 2, and bf16_optimizer params\n",
    "    if hasattr(param, '_hp_mapping'):\n",
    "        return param.get_full_hp_param()\n",
    "    return None\n",
    "\n",
    "\n",
    "# 收集并访问各分区fp16优化器状态量，返回FP32优化器状态量\n",
    "def safe_get_full_optimizer_state(param, optim_state_key):\n",
    "    \"\"\"Assemble and return the fp32 optimizer state of a low-precision (e.g., fp16) parameter.\n",
    "\n",
    "        Args:\n",
    "            param (``torch.nn.Parameter``): A model parameter\n",
    "    \"\"\"\n",
    "    # ZeRO stage 3 param\n",
    "    if hasattr(param, 'ds_id'):\n",
    "        return param._z3_optimizer.get_full_hp_param(param, optim_state_key)\n",
    "\n",
    "    # ZeRO stage 1, 2, and bf16_optimizer params\n",
    "    if hasattr(param, '_hp_mapping'):\n",
    "        return param.get_full_hp_param(optim_state_key)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# 收集并访问各分区fp16梯度，返回fp32梯度\n",
    "# TODO: Figure out the correct return dtype\n",
    "def safe_get_full_grad(param):\n",
    "    \"\"\"Assemble and return the fp32 gradient of a low-precision (e.g., fp16) parameter.\n",
    "\n",
    "        Args:\n",
    "            param (``torch.nn.Parameter``): A model parameter\n",
    "    \"\"\"\n",
    "    if param.grad is not None:\n",
    "        return param.grad\n",
    "\n",
    "    # ZeRO stage 3 param\n",
    "    if hasattr(param, 'ds_id'):\n",
    "        return param._z3_optimizer.get_fp32_grad_for_param(param)\n",
    "\n",
    "    # ZeRO stage 1, 2, and bf16_optimizer params\n",
    "    if hasattr(param, '_hp_mapping'):\n",
    "        return param.get_full_hp_grad()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述debug API应用在训练循环中的示例代码：\n",
    "\n",
    "```py\n",
    "backward(loss)\n",
    "[...]\n",
    "from deepspeed.utils import safe_get_full_fp32_param, safe_get_full_grad, safe_get_full_optimizer_state\n",
    "for n, lp in model.named_parameters():\n",
    "    # 1. gradient lookup\n",
    "    # For zero1 and zero2, gradient lookup must be called after `backward` and before `step`\n",
    "    # For zero3, gradient lookup must be called after `backward`\n",
    "    hp_grad = safe_get_full_grad(lp)\n",
    "\n",
    "    # 2. fp32 and optim states can probably be called anywhere in the training loop, but will be updated after `step`\n",
    "    hp = safe_get_full_fp32_param(lp)\n",
    "    exp_avg = safe_get_full_optimizer_state(lp, \"exp_avg\")\n",
    "    exp_avg_sq = safe_get_full_optimizer_state(lp, \"exp_avg_sq\")\n",
    "\n",
    "[...]\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们针对deepspeed/deepspeed/runtime/zero/stage_1_and_2.py这个文件里面最主要的\n",
    "\n",
    "`class DeepSpeedZeroOptimizer(ZeROOptimizer):`\n",
    "\n",
    "这个类（一共2000多行）的部分函数，分析一下具体的ZeRO优化器的计算是如何进行的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`init`部分先略过，首先看`def initialize_optimizer_states(self):`这个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_optimizer_states(self):\n",
    "\n",
    "    # 先初始化优化器内参数（single_partition_of_fp32_groups）的梯度\n",
    "    for i, group in enumerate(self.bit16_groups):\n",
    "        single_grad_partition = torch.zeros(int(self.partition_size[i]),\n",
    "                                            dtype=self.single_partition_of_fp32_groups[i].dtype,\n",
    "                                            device=self.device)\n",
    "        self.single_partition_of_fp32_groups[i].grad = get_accelerator().pin_memory(\n",
    "            single_grad_partition) if self.cpu_offload else single_grad_partition\n",
    "\n",
    "    # 核心是初始化优化器状态量时，更新一下self.optimizer\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # 如果不需要进行cpu卸载，那么就将梯度置为None，即执行梯度清零\n",
    "    if not self.cpu_offload:\n",
    "        for group in self.single_partition_of_fp32_groups:\n",
    "            group.grad = None  #class init\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是ZeRO-Stage1划分梯度的做法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 622\n",
    "#########################################################################\n",
    "#################### ZeRO Stage 1 - reduce gradients ####################\n",
    "#########################################################################\n",
    "def reduce_gradients(self, pipeline_parallel=False):\n",
    "    world_size = dist.get_world_size(self.dp_process_group)\n",
    "    my_rank = dist.get_rank(self.dp_process_group)\n",
    "\n",
    "    # 使用Pipeline parellel时必须创建ipg_buffer，因为反向处理是在zero之外处理的\n",
    "    if pipeline_parallel and self.contiguous_gradients:\n",
    "        self.ipg_buffer = []\n",
    "        buf_0 = torch.empty(int(self.reduce_bucket_size),\n",
    "                            dtype=self.dtype,\n",
    "                            device=get_accelerator().current_device_name())\n",
    "        self.ipg_buffer.append(buf_0)\n",
    "        self.ipg_index = 0\n",
    "\n",
    "    if not self.overlap_comm:\n",
    "        for i, group in enumerate(self.bit16_groups):\n",
    "            for param in group:\n",
    "                if param.grad is not None:\n",
    "                    # 遍历每个分区的梯度，将其累加到对应的分区中\n",
    "                    self.reduce_ready_partitions_and_remove_grads(param, i)\n",
    "    # 在hook或是non-hook状态下都需要减少所有的pending待定状态下的梯度\n",
    "    self.overlapping_partition_gradients_reduce_epilogue()\n",
    "    \n",
    "# ---------------------上面过程所用到的对应函数（套娃开始）---------------------\n",
    "# Line 1250\n",
    "def reduce_ready_partitions_and_remove_grads(self, param, i):\n",
    "    if self.partition_gradients or self.is_gradient_accumulation_boundary:\n",
    "        self.reduce_independent_p_g_buckets_and_remove_grads(param, i)\n",
    "        \n",
    "# Line 826\n",
    "############### Independent Partition Gradient ########################\n",
    "def reduce_independent_p_g_buckets_and_remove_grads(self, param, i):\n",
    "    # 如果将要加入到 IPG 缓冲区中的梯度的大小和已经在缓冲区中的梯度的大小之和超过了预设的阈值 reduce_bucket_size，则需要对缓冲区中的梯度进行聚合，以释放一部分内存空间。\n",
    "    if self.elements_in_ipg_bucket + param.numel() > self.reduce_bucket_size:\n",
    "        self.report_ipg_memory_usage(\"In ipg_remove_grads before reduce_ipg_grads\", param.numel())\n",
    "        self.reduce_ipg_grads()\n",
    "        if self.contiguous_gradients and self.overlap_comm:\n",
    "            # Swap ipg_index between 0 and 1\n",
    "            self.ipg_index = 1 - self.ipg_index\n",
    "        self.report_ipg_memory_usage(\"In ipg_remove_grads after reduce_ipg_grads\", param.numel())\n",
    "\n",
    "    param_id = self.get_param_id(param)\n",
    "    assert self.params_already_reduced[param_id] == False, \\\n",
    "        f\"The parameter {param_id} has already been reduced. \\\n",
    "        Gradient computed twice for this partition. \\\n",
    "        Multiple gradient reduction is currently not supported\"\n",
    "        \n",
    "    # 如果当前参数的大小超过了 reduce_bucket_size，则认为它是一个特别大的参数，不能加入到 IPG 缓冲区中，而是直接聚合。将它记录在 extra_large_param_to_reduce 变量中，在后续的 backward() 方法中会直接处理。\n",
    "    if param.numel() > self.reduce_bucket_size:\n",
    "        self.extra_large_param_to_reduce = param\n",
    "    # 如果当前参数的大小小于等于 reduce_bucket_size，并且使用连续的梯度缓冲区，则将参数的梯度添加到 IPG 缓冲区的当前索引处，并更新缓冲区中已经存储的梯度的大小。这里使用了 narrow() 方法来从 IPG 缓冲区的当前索引处开始，连续地取出与参数梯度大小相等的一段空间，并将参数梯度的数据复制到这个空间中。这样可以避免内存碎片和梯度展开的问题，同时也能保证梯度在内存中的连续性，以便后续的通信和聚合操作。\n",
    "    elif self.contiguous_gradients:\n",
    "        # keeping the gradients contiguous to prevent memory fragmentation, and avoid flattening\n",
    "        new_grad_tensor = self.ipg_buffer[self.ipg_index].narrow(0, self.elements_in_ipg_bucket, param.numel())\n",
    "        new_grad_tensor.copy_(param.grad.view(-1))\n",
    "        param.grad.data = new_grad_tensor.data.view_as(param.grad)\n",
    "    # ------上面都是一些检查和准备的过程------\n",
    "    \n",
    "    # ------下面是将梯度加入到 IPG 缓冲区中------\n",
    "    # 记录当前缓冲区中已经存储的梯度的大小\n",
    "    self.elements_in_ipg_bucket += param.numel()\n",
    "    \n",
    "    # 如果参数的梯度是 None，则抛出异常，因为无法将 None 梯度加入到 IPG 缓冲区中。\n",
    "    assert param.grad is not None, f\"rank {dist.get_rank()} - Invalid to reduce Param {param_id} with None gradient\"\n",
    "\n",
    "    # 接下来，将参数的梯度添加到 grads_in_ipg_bucket 列表中，并将参数本身和其 ID 添加到 params_in_ipg_bucket 列表中。这两个列表用于在后续的 reduce_ipg_grads() 方法中对梯度进行聚合。\n",
    "    self.grads_in_ipg_bucket.append(param.grad)\n",
    "    self.params_in_ipg_bucket.append((i, param, param_id))\n",
    "\n",
    "    # 给MOE模型的特殊参数做标记\n",
    "    if is_moe_param(param):\n",
    "        self.ipg_bucket_has_moe_params = True\n",
    "\n",
    "    # 最后，调用 report_ipg_memory_usage() 方法记录 IPG 缓冲区的内存使用情况\n",
    "    self.report_ipg_memory_usage(\"End ipg_remove_grads\", 0)\n",
    "    \n",
    "# Line 815\n",
    "def report_ipg_memory_usage(self, tag, param_elems):\n",
    "    elem_count = self.elements_in_ipg_bucket + param_elems\n",
    "    percent_of_bucket_size = (100.0 * elem_count) // self.reduce_bucket_size\n",
    "    see_memory_usage(\n",
    "        f\"{tag}: elems in_bucket {self.elements_in_ipg_bucket} param {param_elems} max_percent {percent_of_bucket_size}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`方法：核心是loss scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, loss, retain_graph=False):\n",
    "    \"\"\"\n",
    "    :attr:`backward` performs the following steps:\n",
    "\n",
    "    1. fp32_loss = loss.float()\n",
    "    2. scaled_loss = fp32_loss*loss_scale\n",
    "    3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's fp16 leaves\n",
    "    \"\"\"\n",
    "    self.micro_step_id += 1\n",
    "\n",
    "    # 如果启用了 contiguous_gradients，则会创建一个 IPG 缓冲区（详见之前的解释），并将当前使用的 IPG 缓冲区的索引设置为 0。\n",
    "    if self.contiguous_gradients:\n",
    "        self.ipg_buffer = []\n",
    "        buf_0 = torch.empty(int(self.reduce_bucket_size),\n",
    "                            dtype=self.dtype,\n",
    "                            device=get_accelerator().current_device_name())\n",
    "        self.ipg_buffer.append(buf_0)\n",
    "\n",
    "        # Use double buffers to avoid data access conflict when overlap_comm is enabled.\n",
    "        if self.overlap_comm:\n",
    "            buf_1 = torch.empty(int(self.reduce_bucket_size),\n",
    "                                dtype=self.dtype,\n",
    "                                device=get_accelerator().current_device_name())\n",
    "            self.ipg_buffer.append(buf_1)\n",
    "        self.ipg_index = 0\n",
    "\n",
    "    # 如果启用了 custom_loss_scaler，则使用外部提供的损失缩放因子（即 external_loss_scale）来缩放损失值，并进行反向传播。否则，使用内置的损失缩放器 loss_scaler 来进行反向传播。\n",
    "    if self.custom_loss_scaler:\n",
    "        scaled_loss = self.external_loss_scale * loss\n",
    "        scaled_loss.backward()\n",
    "    else:\n",
    "        self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step()`方法：重点要关注的部分，在正常torch optimizer的`step()`之上，加入对FP8 utils的各类判断和计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先贴一下check_overflow()的源码\n",
    "# Line 1892\n",
    "def check_overflow(self, partition_gradients=True):\n",
    "    self._check_overflow(partition_gradients)\n",
    "    \n",
    "# Line 1797\n",
    "def _check_overflow(self, partition_gradients=True):\n",
    "    self.overflow = self.has_overflow(partition_gradients)\n",
    "    \n",
    "# Line 1815\n",
    "def has_overflow(self, partition_gradients=True):\n",
    "    if partition_gradients:\n",
    "        overflow = self.local_overflow if self.cpu_offload else self.has_overflow_partitioned_grads_serial()\n",
    "        overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
    "        # 这将捕获所有数据并行和专家并行进程的溢出，因为专家并行进程是数据并行进程的一个子集\n",
    "        dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.dp_process_group)\n",
    "\n",
    "    else:\n",
    "        params = []\n",
    "        for group in self.bit16_groups:\n",
    "            for param in group:\n",
    "                params.append(param)\n",
    "\n",
    "        overflow = self.has_overflow_serial(params, is_grad_list=partition_gradients)\n",
    "        overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
    "\n",
    "    # 模型并行时每张GPU只装载了模型的一部分，因此需要all_reduce来同步所有的模型并行GPU上的overflow标志\n",
    "    self._model_parallel_all_reduce(tensor=overflow_gpu, op=dist.ReduceOp.MAX)\n",
    "\n",
    "    overflow = overflow_gpu[0].item()\n",
    "    return bool(overflow)\n",
    "    \n",
    "def has_overflow_serial(self, params, is_grad_list=False):\n",
    "    # 序列化遍历所有的参数，检查他们是不是有inf或者nan\n",
    "    for p in params:\n",
    "        if p.grad is not None and self._has_inf_or_nan(p.grad.data):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "    \n",
    "# Line 1839   \n",
    "# `x` is a torch.Tensor\n",
    "@staticmethod\n",
    "def _has_inf_or_nan(x, j=None):\n",
    "    try:\n",
    "        # 如果x是半精度half格式，那么.float()会产生一个额外的全精度格式的深度拷贝，但在以下情况下是必要的\n",
    "        # Pytorch的.sum()创建了一个与x类型相同的单元素张量（对于某些最新版本的pytorch来说是这样的）。\n",
    "        cpu_sum = float(x.float().sum())\n",
    "        # 如果.sum()返回一个Python标量，可以使用更有效的版本\n",
    "        # cpu_sum = float(x.sum())\n",
    "    except RuntimeError as instance:\n",
    "        # 我们要检查这个instance的异常实例是不是真的来自于溢出异常，而不是其他的异常\n",
    "        if \"value cannot be converted\" not in instance.args[0]:\n",
    "            raise\n",
    "        return True\n",
    "    else:\n",
    "        # 对于正常浮点数格式，直接按torch的float类判断即可\n",
    "        if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 1636\n",
    "def step(self, closure=None):\n",
    "    \"\"\"\n",
    "    Not supporting closure.\n",
    "    \"\"\"\n",
    "    self.micro_step_id = -1\n",
    "\n",
    "    see_memory_usage(f\"In step before checking overflow\")\n",
    "\n",
    "    # 首先对各组的参数计算范数，以此来判断是否发生了overflow上溢出\n",
    "    self.check_overflow()\n",
    "    OPTIMIZER_ALLGATHER = 'optimizer_allgather'\n",
    "    OPTIMIZER_GRADIENTS = 'optimizer_gradients'\n",
    "    OPTIMIZER_STEP = 'optimizer_step'\n",
    "    timer_names = [OPTIMIZER_ALLGATHER, OPTIMIZER_GRADIENTS, OPTIMIZER_STEP]\n",
    "\n",
    "    prev_scale = self.loss_scale\n",
    "    self._update_scale(self.overflow)\n",
    "    if self.overflow:\n",
    "        see_memory_usage('After overflow before clearing gradients')\n",
    "        self.zero_grad(set_to_none=True)\n",
    "        if self.cpu_offload:\n",
    "            self.reset_cpu_buffers()\n",
    "        else:\n",
    "            self.averaged_gradients = {}\n",
    "\n",
    "        see_memory_usage('After overflow after clearing gradients')\n",
    "\n",
    "        self.start_timers(timer_names)\n",
    "        self.stop_timers(timer_names)\n",
    "        return\n",
    "\n",
    "    # 第1步：使用fp-16 grads计算梯度范数\n",
    "    see_memory_usage('Before norm calculation')\n",
    "    scaled_global_grad_norm = self.scaled_global_norm()\n",
    "    self._global_grad_norm = scaled_global_grad_norm / prev_scale\n",
    "\n",
    "    see_memory_usage('After norm before optimizer')\n",
    "    # 第2步：同时运行优化器和upscaling\n",
    "    for i, group in enumerate(self.bit16_groups):\n",
    "        self.start_timers([OPTIMIZER_GRADIENTS])\n",
    "        partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n",
    "        if self.cpu_offload:\n",
    "            single_grad_partition = self.single_partition_of_fp32_groups[i].grad\n",
    "            self.unscale_and_clip_grads([single_grad_partition], scaled_global_grad_norm)\n",
    "            self.stop_timers([OPTIMIZER_GRADIENTS])\n",
    "            self.start_timers([OPTIMIZER_STEP])\n",
    "            self._optimizer_step(i)\n",
    "\n",
    "            bit16_partitions = self.parallel_partitioned_bit16_groups[i]\n",
    "            fp32_partition = self.single_partition_of_fp32_groups[i]\n",
    "            bit16_partitions[partition_id].data.copy_(fp32_partition.data)\n",
    "\n",
    "            self.stop_timers([OPTIMIZER_STEP])\n",
    "        else:\n",
    "            # 所有没有被这个过程更新的参数的自由梯度(ZeRO stage2)\n",
    "            self.free_grad_in_param_list(self.params_not_in_partition[i])\n",
    "\n",
    "            # 为这个过程中更新的参数创建一个flatten展平后的梯度\n",
    "            # 如果我们是最后一个分区，确保我们有相同大小的梯度和分区大小，如果没有，就用0填充\n",
    "            if partition_id == dist.get_world_size(group=self.real_dp_process_group[i]) - 1:\n",
    "                single_grad_partition = self.flatten_dense_tensors_aligned(\n",
    "                    self.averaged_gradients[i],\n",
    "                    int(self.partition_size[i])).to(self.single_partition_of_fp32_groups[i].dtype)\n",
    "            else:\n",
    "                single_grad_partition = self.flatten(self.averaged_gradients[i]).to(\n",
    "                    self.single_partition_of_fp32_groups[i].dtype)\n",
    "            assert single_grad_partition.numel() == self.partition_size[i], \\\n",
    "                \"averaged gradients have different number of elements that partition size {} {} {} {}\".format(\n",
    "                    single_grad_partition.numel(), self.partition_size[i], i, partition_id)\n",
    "\n",
    "            self.single_partition_of_fp32_groups[i].grad = single_grad_partition\n",
    "            # 释放所有的梯度，因为我们已经在dp_grad_partition(ZeRO stage2)中创建了一个必要的副本。\n",
    "            self.free_grad_in_param_list(self.params_in_partition[i])\n",
    "\n",
    "            self.averaged_gradients[i] = None\n",
    "\n",
    "            self.unscale_and_clip_grads([single_grad_partition], scaled_global_grad_norm)\n",
    "            self.stop_timers([OPTIMIZER_GRADIENTS])\n",
    "\n",
    "            # Step 3: 如果没有off_load，那么就运行optimizer_step\n",
    "            self.start_timers([OPTIMIZER_STEP])\n",
    "            self._optimizer_step(i)\n",
    "            '''\n",
    "            def _optimizer_step(self, group_no):\n",
    "                original_param_groups = self.optimizer.param_groups\n",
    "                self.optimizer.param_groups = [original_param_groups[group_no]]\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.param_groups = original_param_groups\n",
    "            '''\n",
    "            # Step 4: 删除fp32梯度的中间变量，因为我们已经不再需要它了\n",
    "            self.single_partition_of_fp32_groups[i].grad = None\n",
    "            del single_grad_partition\n",
    "            bit16_partitions = self.parallel_partitioned_bit16_groups[i]\n",
    "            fp32_partition = self.single_partition_of_fp32_groups[i]\n",
    "            bit16_partitions[partition_id].data.copy_(fp32_partition.data)\n",
    "            self.stop_timers([OPTIMIZER_STEP])\n",
    "\n",
    "    see_memory_usage('After optimizer before all-gather')\n",
    "    if self.cpu_offload:\n",
    "        self.reset_cpu_buffers()\n",
    "\n",
    "    self.start_timers([OPTIMIZER_ALLGATHER])\n",
    "    # 收集各张GPU更新后的权重。\n",
    "    # 然后所有分区的模型参数都被更新，为下一轮forward过程做好准备\n",
    "    all_gather_dp_groups(partitioned_param_groups=self.parallel_partitioned_bit16_groups,\n",
    "                            dp_process_group=self.real_dp_process_group,\n",
    "                            start_alignment_factor=self.nccl_start_alignment_factor,\n",
    "                            allgather_bucket_size=self.allgather_bucket_size)\n",
    "\n",
    "    self.stop_timers([OPTIMIZER_ALLGATHER])\n",
    "\n",
    "    # TODO: we probably don't need this? just to be safe\n",
    "    for i in range(len(self.bit16_groups)):\n",
    "        self._update_model_bit16_weights(i)\n",
    "\n",
    "    self.log_timers(timer_names)\n",
    "    see_memory_usage('After zero_optimizer step')\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于对优化器状态量做不同卡之间的通讯更新的这个函数`all_gather_dp_groups`，我们也有必要拿出来分析一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in deepspeed/deepspeed/runtime/utils.py, Line 918\n",
    "def all_gather_dp_groups(partitioned_param_groups, dp_process_group, start_alignment_factor, allgather_bucket_size):\n",
    "    for group_id, partitioned_params in enumerate(partitioned_param_groups):\n",
    "        # Sequential AllGather Best of both worlds\n",
    "        # 顺序式all_gather操作\n",
    "        partition_id = dist.get_rank(group=dp_process_group[group_id])\n",
    "        dp_world_size = dist.get_world_size(group=dp_process_group[group_id])\n",
    "\n",
    "        # 这部分是性能优化，为了减少allgather的次数，将参数分成多个shard，每个shard的大小为allgather_bucket_size\n",
    "        num_shards = max(1, partitioned_params[partition_id].numel() * dp_world_size // allgather_bucket_size)\n",
    "        # 此外，为了避免一次传输的数据量过大，代码还对梯度分片的数量进行了限制，确保每次传输的数据量在一个可接受的范围内。\n",
    "        shard_size = partitioned_params[partition_id].numel() // num_shards\n",
    "\n",
    "        # 为了保证每个shard的大小都是start_alignment_factor的整数倍，需要对shard_size进行调整，即在分片的时候考虑nccl/rccl的对齐要求\n",
    "        shard_size = shard_size - (shard_size % start_alignment_factor)\n",
    "        num_elements = shard_size\n",
    "        assert shard_size * num_shards <= partitioned_params[partition_id].numel()\n",
    "\n",
    "        # shard: 陶瓷、玻璃等的碎片；碎片；分片；裂片\n",
    "        # 遍历每一个shard_id，将分片后大小相近的参数分片发送给其他进程，并接收其他进程中的梯度分片，最终得到完整的梯度\n",
    "        for shard_id in range(num_shards):\n",
    "\n",
    "            if shard_id == (num_shards - 1):\n",
    "                num_elements = partitioned_params[partition_id].numel() - shard_id * shard_size\n",
    "\n",
    "            shard_list = []\n",
    "            for dp_id in range(dp_world_size):\n",
    "                curr_shard = partitioned_params[dp_id].narrow(0, shard_id * shard_size, num_elements).detach()\n",
    "                shard_list.append(curr_shard)\n",
    "\n",
    "            # 调用dist.all_gather函数来实现参数在多张GPU上的收集\n",
    "            dist.all_gather(shard_list, shard_list[partition_id], dp_process_group[group_id])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
