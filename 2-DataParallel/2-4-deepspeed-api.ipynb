{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeed API\n",
    "\n",
    "这一节介绍DeepSpeed所提供的API，主要关注ZeRO零冗余优化器\n",
    "\n",
    "零冗余优化器 (ZeRO) 通过在分布式代码运行中将三个模型状态（优化器状态、梯度和参数）划分到不同GPU上来消除并行进程的内存冗余。通过这样做，与单纯数据并行相比提高了内存效率，同时保留了其计算粒度和通信效率。\n",
    "\n",
    "1. **ZeRO Stage 1**：优化器状态（例如，对于 Adam 优化器、32 位权重以及一阶和二阶矩估计）在进程之间进行分区，以便每个进程仅更新其分区中的那一部分。\n",
    "\n",
    "2. **ZeRO Stage 2**：用于更新模型权重的减少的 32 位梯度也被分区，这样每个进程只保留与其优化器状态部分对应的梯度。\n",
    "\n",
    "3. **ZeRO Stage 3**：16 位模型参数跨进程分区。 ZeRO-3 将在前向和后向传递过程中自动收集和划分它们。\n",
    "\n",
    "此外，ZeRO-3 包含infinity offload engine以形成 ZeRO-Infinity（[论文](https://arxiv.org/abs/2104.07857)），它可以将所有模型状态卸载到 CPU 和 NVMe 内存，以获得巨大的内存节约。如需深入了解算法，可以参阅有关论文。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO config\n",
    "\n",
    "更改ZeRO优化器各种设置的一个重要文件便是其config文件。完整config选项可见源码deepspeed.runtime.zero.config.DeepSpeedZeroConfig文件，这里只列举比较重要的几项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroStageEnum(int, Enum):\n",
    "    \"\"\" Enum class for possible zero stages \"\"\"\n",
    "    disabled = 0\n",
    "    optimizer_states = 1\n",
    "    gradients = 2\n",
    "    weights = 3\n",
    "    max_stage = 3\n",
    "\n",
    "class DeepSpeedZeroConfig(DeepSpeedConfigModel):\n",
    "    \"\"\"\n",
    "    Sets parameters for ZeRO optimizations.\n",
    "    \"\"\"\n",
    "\n",
    "    stage: ZeroStageEnum = 0\n",
    "    \"\"\"\n",
    "    0：关闭  1：划分优化器状态  2：划分优化器状态和梯度状态  3：划分优化器状态、梯度状态和参数状态\n",
    "    \"\"\"\n",
    "\n",
    "    contiguous_gradients: bool = True\n",
    "    \"\"\"\n",
    "    将梯度复制到连续缓冲区中，以便在反向传播过程中避免内存碎片\n",
    "    \"\"\"\n",
    "\n",
    "    reduce_scatter: bool = True\n",
    "    \"\"\"\n",
    "    在梯度平均时，使用reduce或reduce scatter而不是allreduce\n",
    "    \"\"\"\n",
    "\n",
    "    offload_param: Optional[DeepSpeedZeroOffloadParamConfig] = None\n",
    "    \"\"\"\n",
    "    启用将模型参数卸载到 CPU 或 NVMe。这可以为更大的模型或批量大小释放 GPU 内存。仅对Stage 3 有效。\n",
    "    \"\"\"\n",
    "\n",
    "    offload_optimizer: Optional[DeepSpeedZeroOffloadOptimizerConfig] = None\n",
    "    \"\"\"\n",
    "    启用将优化器状态卸载到 CPU 或 NVMe。这可以为更大的模型或批量大小释放 GPU 内存。对Stage 1、2、3 有效。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    gather_16bit_weights_on_model_save: bool = Field(False, alias=\"stage3_gather_16bit_weights_on_model_save\")\n",
    "    \"\"\"\n",
    "    在通过 save_16bit_model() 保存模型之前合并权重。由于权重是跨 GPU 分区的，它们不是 state_dict 的一部分，因此此函数会在启用此选项时自动收集权重，然后保存 fp16 模型权重。\n",
    "    \"\"\"\n",
    "\n",
    "    # Validators\n",
    "    @validator(\"overlap_comm\")\n",
    "    def overlap_comm_valid(cls, field_value, values):\n",
    "        if field_value is None:\n",
    "            assert (\"stage\" in values), \"DeepSpeedZeroConfig: 'stage' must be defined before 'overlap_comm'\"\n",
    "            field_value = values[\"stage\"] == ZeroStageEnum.weights\n",
    "        return field_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于ZeRO config，还有关于Offload的两个类：\n",
    "\n",
    "`classdeepspeed.runtime.zero.config.DeepSpeedZeroOffloadParamConfig`\n",
    "\n",
    "`classdeepspeed.runtime.zero.config.DeepSpeedZeroOffloadOptimizerConfig`\n",
    "\n",
    "分别是关于参数和优化器状态的offload，这里先忽略之\n",
    "\n",
    "在config.json文件里面所撰写的ZeRO优化器设置大致包括：\n",
    "\n",
    "```json\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": [0|1|2|3],\n",
    "    \"allgather_partitions\": [true|false],\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": false,\n",
    "    \"reduce_scatter\": [true|false],\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"contiguous_gradients\" : [true|false],\n",
    "    \"offload_param\": {\n",
    "      ...\n",
    "    },\n",
    "    \"offload_optimizer\": {\n",
    "      ...\n",
    "    },\n",
    "    \"stage3_max_live_parameters\" : 1e9,\n",
    "    \"stage3_max_reuse_distance\" : 1e9,\n",
    "    \"stage3_prefetch_bucket_size\" : 5e8,\n",
    "    \"stage3_param_persistence_threshold\" : 1e6,\n",
    "    \"sub_group_size\" : 1e12,\n",
    "    \"elastic_checkpoint\" : [true|false],\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": [true|false],\n",
    "    \"ignore_unused_parameters\": [true|false]\n",
    "    \"round_robin_gradients\": [true|false]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO Debugging\n",
    "\n",
    "如果想要在debug的过程中访问模型参数、梯度和优化器状态，DeepSpeed 提供了以下API来访问他们，其中各参数都是以未被分区的形式来呈现的。\n",
    "\n",
    "**重要提示**：请注意，参加training所有进程都必须调用这些实用程序，即使我们只想在主进程中对结果执行某些操作。如果没有所有进程都参与，这些API将被挂起。\n",
    "\n",
    "此外，必须在特定的阶段访问正确的参数。例如，梯度在`backward`之后、`step`之前有效。优化器状态在`step`之后更新。 fp32 主权重也是如此。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in deepspeed/utils/tensor_fragment.py\n",
    "\n",
    "# 收集并访问各分区fp16参数，返回fp32参数\n",
    "def safe_get_full_fp32_param(param):\n",
    "    \"\"\"Assemble and return the fp32 parameter of a low-precision (e.g., fp16) parameter.\n",
    "\n",
    "        Args:\n",
    "            param (``torch.nn.Parameter``): A model parameter\n",
    "    \"\"\"\n",
    "    # ZeRO stage 3 param\n",
    "    if hasattr(param, 'ds_id'):\n",
    "        return param._z3_optimizer.get_full_hp_param(param)\n",
    "\n",
    "    # ZeRO stage 1, 2, and bf16_optimizer params\n",
    "    if hasattr(param, '_hp_mapping'):\n",
    "        return param.get_full_hp_param()\n",
    "    return None\n",
    "\n",
    "\n",
    "# 收集并访问各分区fp16优化器状态量，返回FP32优化器状态量\n",
    "def safe_get_full_optimizer_state(param, optim_state_key):\n",
    "    \"\"\"Assemble and return the fp32 optimizer state of a low-precision (e.g., fp16) parameter.\n",
    "\n",
    "        Args:\n",
    "            param (``torch.nn.Parameter``): A model parameter\n",
    "    \"\"\"\n",
    "    # ZeRO stage 3 param\n",
    "    if hasattr(param, 'ds_id'):\n",
    "        return param._z3_optimizer.get_full_hp_param(param, optim_state_key)\n",
    "\n",
    "    # ZeRO stage 1, 2, and bf16_optimizer params\n",
    "    if hasattr(param, '_hp_mapping'):\n",
    "        return param.get_full_hp_param(optim_state_key)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# 收集并访问各分区fp16梯度，返回fp32梯度\n",
    "# TODO: Figure out the correct return dtype\n",
    "def safe_get_full_grad(param):\n",
    "    \"\"\"Assemble and return the fp32 gradient of a low-precision (e.g., fp16) parameter.\n",
    "\n",
    "        Args:\n",
    "            param (``torch.nn.Parameter``): A model parameter\n",
    "    \"\"\"\n",
    "    if param.grad is not None:\n",
    "        return param.grad\n",
    "\n",
    "    # ZeRO stage 3 param\n",
    "    if hasattr(param, 'ds_id'):\n",
    "        return param._z3_optimizer.get_fp32_grad_for_param(param)\n",
    "\n",
    "    # ZeRO stage 1, 2, and bf16_optimizer params\n",
    "    if hasattr(param, '_hp_mapping'):\n",
    "        return param.get_full_hp_grad()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述debug API应用在训练循环中的示例代码：\n",
    "\n",
    "```py\n",
    "backward(loss)\n",
    "[...]\n",
    "from deepspeed.utils import safe_get_full_fp32_param, safe_get_full_grad, safe_get_full_optimizer_state\n",
    "for n, lp in model.named_parameters():\n",
    "    # 1. gradient lookup\n",
    "    # For zero1 and zero2, gradient lookup must be called after `backward` and before `step`\n",
    "    # For zero3, gradient lookup must be called after `backward`\n",
    "    hp_grad = safe_get_full_grad(lp)\n",
    "\n",
    "    # 2. fp32 and optim states can probably be called anywhere in the training loop, but will be updated after `step`\n",
    "    hp = safe_get_full_fp32_param(lp)\n",
    "    exp_avg = safe_get_full_optimizer_state(lp, \"exp_avg\")\n",
    "    exp_avg_sq = safe_get_full_optimizer_state(lp, \"exp_avg_sq\")\n",
    "\n",
    "[...]\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们针对deepspeed/deepspeed/runtime/zero/stage_1_and_2.py这个文件里面最主要的\n",
    "\n",
    "`class DeepSpeedZeroOptimizer(ZeROOptimizer):`\n",
    "\n",
    "这个类（一共2000多行）的部分函数，分析一下具体的ZeRO优化器的计算是如何进行的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`init`部分先略过，首先看`def initialize_optimizer_states(self):`这个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_optimizer_states(self):\n",
    "\n",
    "    # 先初始化优化器内参数（single_partition_of_fp32_groups）的梯度\n",
    "    for i, group in enumerate(self.bit16_groups):\n",
    "        single_grad_partition = torch.zeros(int(self.partition_size[i]),\n",
    "                                            dtype=self.single_partition_of_fp32_groups[i].dtype,\n",
    "                                            device=self.device)\n",
    "        self.single_partition_of_fp32_groups[i].grad = get_accelerator().pin_memory(\n",
    "            single_grad_partition) if self.cpu_offload else single_grad_partition\n",
    "\n",
    "    # 核心是初始化优化器状态量时，更新一下self.optimizer\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # 如果不需要进行cpu卸载，那么就将梯度置为None，即执行梯度清零\n",
    "    if not self.cpu_offload:\n",
    "        for group in self.single_partition_of_fp32_groups:\n",
    "            group.grad = None  #class init\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是ZeRO-Stage1划分梯度的做法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 622\n",
    "#########################################################################\n",
    "#################### ZeRO Stage 1 - reduce gradients ####################\n",
    "#########################################################################\n",
    "def reduce_gradients(self, pipeline_parallel=False):\n",
    "    world_size = dist.get_world_size(self.dp_process_group)\n",
    "    my_rank = dist.get_rank(self.dp_process_group)\n",
    "\n",
    "    # 使用Pipeline parellel时必须创建ipg_buffer，因为反向处理是在zero之外处理的\n",
    "    if pipeline_parallel and self.contiguous_gradients:\n",
    "        self.ipg_buffer = []\n",
    "        buf_0 = torch.empty(int(self.reduce_bucket_size),\n",
    "                            dtype=self.dtype,\n",
    "                            device=get_accelerator().current_device_name())\n",
    "        self.ipg_buffer.append(buf_0)\n",
    "        self.ipg_index = 0\n",
    "\n",
    "    if not self.overlap_comm:\n",
    "        for i, group in enumerate(self.bit16_groups):\n",
    "            for param in group:\n",
    "                if param.grad is not None:\n",
    "                    # 遍历每个分区的梯度，将其累加到对应的分区中\n",
    "                    self.reduce_ready_partitions_and_remove_grads(param, i)\n",
    "    # 在hook或是non-hook状态下都需要减少所有的pending待定状态下的梯度\n",
    "    self.overlapping_partition_gradients_reduce_epilogue()\n",
    "    \n",
    "# ---------------------上面过程所用到的对应函数（套娃开始）---------------------\n",
    "# Line 1250\n",
    "def reduce_ready_partitions_and_remove_grads(self, param, i):\n",
    "    if self.partition_gradients or self.is_gradient_accumulation_boundary:\n",
    "        self.reduce_independent_p_g_buckets_and_remove_grads(param, i)\n",
    "        \n",
    "# Line 826\n",
    "############### Independent Partition Gradient ########################\n",
    "def reduce_independent_p_g_buckets_and_remove_grads(self, param, i):\n",
    "    # 如果将要加入到 IPG 缓冲区中的梯度的大小和已经在缓冲区中的梯度的大小之和超过了预设的阈值 reduce_bucket_size，则需要对缓冲区中的梯度进行聚合，以释放一部分内存空间。\n",
    "    if self.elements_in_ipg_bucket + param.numel() > self.reduce_bucket_size:\n",
    "        self.report_ipg_memory_usage(\"In ipg_remove_grads before reduce_ipg_grads\", param.numel())\n",
    "        self.reduce_ipg_grads()\n",
    "        if self.contiguous_gradients and self.overlap_comm:\n",
    "            # Swap ipg_index between 0 and 1\n",
    "            self.ipg_index = 1 - self.ipg_index\n",
    "        self.report_ipg_memory_usage(\"In ipg_remove_grads after reduce_ipg_grads\", param.numel())\n",
    "\n",
    "    param_id = self.get_param_id(param)\n",
    "    assert self.params_already_reduced[param_id] == False, \\\n",
    "        f\"The parameter {param_id} has already been reduced. \\\n",
    "        Gradient computed twice for this partition. \\\n",
    "        Multiple gradient reduction is currently not supported\"\n",
    "        \n",
    "    # 如果当前参数的大小超过了 reduce_bucket_size，则认为它是一个特别大的参数，不能加入到 IPG 缓冲区中，而是直接聚合。将它记录在 extra_large_param_to_reduce 变量中，在后续的 backward() 方法中会直接处理。\n",
    "    if param.numel() > self.reduce_bucket_size:\n",
    "        self.extra_large_param_to_reduce = param\n",
    "    # 如果当前参数的大小小于等于 reduce_bucket_size，并且使用连续的梯度缓冲区，则将参数的梯度添加到 IPG 缓冲区的当前索引处，并更新缓冲区中已经存储的梯度的大小。这里使用了 narrow() 方法来从 IPG 缓冲区的当前索引处开始，连续地取出与参数梯度大小相等的一段空间，并将参数梯度的数据复制到这个空间中。这样可以避免内存碎片和梯度展开的问题，同时也能保证梯度在内存中的连续性，以便后续的通信和聚合操作。\n",
    "    elif self.contiguous_gradients:\n",
    "        # keeping the gradients contiguous to prevent memory fragmentation, and avoid flattening\n",
    "        new_grad_tensor = self.ipg_buffer[self.ipg_index].narrow(0, self.elements_in_ipg_bucket, param.numel())\n",
    "        new_grad_tensor.copy_(param.grad.view(-1))\n",
    "        param.grad.data = new_grad_tensor.data.view_as(param.grad)\n",
    "    # ------上面都是一些检查和准备的过程------\n",
    "    \n",
    "    # ------下面是将梯度加入到 IPG 缓冲区中------\n",
    "    # 记录当前缓冲区中已经存储的梯度的大小\n",
    "    self.elements_in_ipg_bucket += param.numel()\n",
    "    \n",
    "    # 如果参数的梯度是 None，则抛出异常，因为无法将 None 梯度加入到 IPG 缓冲区中。\n",
    "    assert param.grad is not None, f\"rank {dist.get_rank()} - Invalid to reduce Param {param_id} with None gradient\"\n",
    "\n",
    "    # 接下来，将参数的梯度添加到 grads_in_ipg_bucket 列表中，并将参数本身和其 ID 添加到 params_in_ipg_bucket 列表中。这两个列表用于在后续的 reduce_ipg_grads() 方法中对梯度进行聚合。\n",
    "    self.grads_in_ipg_bucket.append(param.grad)\n",
    "    self.params_in_ipg_bucket.append((i, param, param_id))\n",
    "\n",
    "    # 给MOE模型的特殊参数做标记\n",
    "    if is_moe_param(param):\n",
    "        self.ipg_bucket_has_moe_params = True\n",
    "\n",
    "    # 最后，调用 report_ipg_memory_usage() 方法记录 IPG 缓冲区的内存使用情况\n",
    "    self.report_ipg_memory_usage(\"End ipg_remove_grads\", 0)\n",
    "    \n",
    "# Line 815\n",
    "def report_ipg_memory_usage(self, tag, param_elems):\n",
    "    elem_count = self.elements_in_ipg_bucket + param_elems\n",
    "    percent_of_bucket_size = (100.0 * elem_count) // self.reduce_bucket_size\n",
    "    see_memory_usage(\n",
    "        f\"{tag}: elems in_bucket {self.elements_in_ipg_bucket} param {param_elems} max_percent {percent_of_bucket_size}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
