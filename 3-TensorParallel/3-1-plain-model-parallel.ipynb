{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型并行\n",
    "\n",
    "在模型非常大，单张GPU无法承载所有的模型参数时，就需要使用模型并行。根据Megatron-LM和GPipe的论文，模型并行主要分为横向划分模型参数的张量并行（Tensor Parallelism）和纵向划分参数的流水线并行（Pipeline Parallelism）\n",
    "\n",
    "在这一节中我们主要依据Pytorch官方的[tutorials](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)，来给出模型并行的简单示例。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们定义一个简单的网络。我们采用最笨的办法，在网络定义时就讲第一个线性层放在第一张GPU上，第二个线性层放在第二个GPU上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.net1 = nn.Linear(4, 6).to('cuda:0')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(6, 2).to('cuda:1')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 在模型的前向过程中，需要将输入分配到不同的设备上\n",
    "        x = self.relu(self.net1(x.to('cuda:0')))\n",
    "        return self.net2(x.to('cuda:1'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，在模型前向过程中不需要额外的修改，`backward()`函数和`step()`函数会自动处理多个模型上的梯度，就好像模型就在同一张卡上一样\n",
    "\n",
    "唯一需要注意的是，计算损失函数时需要保证函数的输出output和标签label是在同一张卡上的，这个例子这里便是`cuda:1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1: Parameter containing:\n",
      "tensor([[-0.3877,  0.3361, -0.4700, -0.1861],\n",
      "        [-0.2826,  0.2885,  0.3352, -0.2339],\n",
      "        [ 0.1411,  0.2614,  0.0342,  0.3235],\n",
      "        [-0.3217,  0.3397, -0.0691,  0.3553],\n",
      "        [ 0.3835, -0.3737,  0.0907,  0.2470],\n",
      "        [ 0.0396,  0.2315,  0.4344,  0.3013]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "net2: Parameter containing:\n",
      "tensor([[-0.2344, -0.3099,  0.2224,  0.3760,  0.1694, -0.2281],\n",
      "        [-0.2098, -0.1935,  0.3041,  0.1562, -0.3340, -0.2639]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "---------After update---------\n",
      "net1: Parameter containing:\n",
      "tensor([[-0.3876,  0.3361, -0.4700, -0.1861],\n",
      "        [-0.2825,  0.2885,  0.3352, -0.2339],\n",
      "        [ 0.1411,  0.2614,  0.0341,  0.3235],\n",
      "        [-0.3217,  0.3397, -0.0692,  0.3553],\n",
      "        [ 0.3834, -0.3736,  0.0907,  0.2471],\n",
      "        [ 0.0397,  0.2315,  0.4344,  0.3012]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "net2: Parameter containing:\n",
      "tensor([[-0.2343, -0.3098,  0.2224,  0.3760,  0.1691, -0.2281],\n",
      "        [-0.2098, -0.1934,  0.3041,  0.1562, -0.3338, -0.2638]],\n",
      "       device='cuda:1', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = ToyModel()\n",
    "\n",
    "print(f\"net1: {model.net1.weight}\")\n",
    "print(f\"net2: {model.net2.weight}\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "outputs = model(torch.randn(20, 4))\n",
    "labels = torch.randn(20, 2).to('cuda:1')\n",
    "loss_fn(outputs, labels).backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"---------After update---------\")\n",
    "print(f\"net1: {model.net1.weight}\")\n",
    "print(f\"net2: {model.net2.weight}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以继承EXP-2的例子，将之前训练cifar-10的代码使用上面的方法，将模型的不同部分分配到不同的GPU上，然后进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下是一个手动将不同层分配到不同GPU上的ConvNet。这里我们使用了四张GPU，每张GPU上分配了一个层。\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)).to('cuda:0')\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)).to('cuda:1')\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()).to('cuda:2')\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(84, num_classes),\n",
    "            nn.Softmax(dim=1)).to('cuda:3')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.to('cuda:0'))\n",
    "        x = self.conv2(x.to('cuda:1'))\n",
    "        x = self.fc(x.to('cuda:2'))\n",
    "        return self.out(x.to('cuda:3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查一下模型不同层所在的设备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 in cuda:0\n",
      "conv2 in cuda:1\n",
      "fc in cuda:2\n",
      "out in cuda:3\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "print(f\"conv1 in {model.conv1[0].weight.device}\")\n",
    "print(f\"conv2 in {model.conv2[0].weight.device}\")\n",
    "print(f\"fc in {model.fc[1].weight.device}\")\n",
    "print(f\"out in {model.out[0].weight.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后执行简单的训练过程，看看是否能够正常运行。下面这段代码和2-DataParallel中的内容一致，而且删去了所有数据并行的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifar dataset already exist in '../2-DataParallel/data', skip download\n",
      "Epoch: 0, Loss: 2.13, acc: 0.33, time cost: 15.83s\n",
      "Epoch: 1, Loss: 2.03, acc: 0.42, time cost: 15.11s\n",
      "Epoch: 2, Loss: 2.00, acc: 0.46, time cost: 14.49s\n",
      "Epoch: 3, Loss: 1.97, acc: 0.49, time cost: 14.94s\n",
      "Epoch: 4, Loss: 1.95, acc: 0.51, time cost: 14.37s\n",
      "Epoch: 5, Loss: 1.93, acc: 0.53, time cost: 14.98s\n",
      "Epoch: 6, Loss: 1.91, acc: 0.55, time cost: 14.59s\n",
      "Epoch: 7, Loss: 1.90, acc: 0.56, time cost: 15.24s\n",
      "Epoch: 8, Loss: 1.89, acc: 0.57, time cost: 15.05s\n",
      "Epoch: 9, Loss: 1.88, acc: 0.58, time cost: 15.04s\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "\n",
    "def get_dataset(path='./data'):\n",
    "    DOWNLOAD = False\n",
    "    if not(os.path.exists(path)) or not os.listdir(path):\n",
    "    # not cifar dir or cifar is empyt dir\n",
    "        DOWNLOAD = True\n",
    "    else:\n",
    "        print(\"Cifar dataset already exist in '{}', skip download\".format(path))\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root = path,\n",
    "        train = True,\n",
    "        transform = transform,\n",
    "        download = DOWNLOAD\n",
    "    )\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root = path,\n",
    "        train = False,\n",
    "        transform = transform,\n",
    "        download = DOWNLOAD\n",
    "    )\n",
    "    \n",
    "    return trainset, testset\n",
    "\n",
    "def main():\n",
    "    net = ConvNet()\n",
    "\n",
    "    trainset, testset = get_dataset(\"../2-DataParallel/data\")\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    for epoch in range(10):\n",
    "        t0 = time.time()\n",
    "        net.train()\n",
    "        \n",
    "        loss_sum,acc_sum = 0,0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            \n",
    "            #! 这里，输入张量在cuda:0上，标签张量在cuda:3上\n",
    "            inputs, labels = inputs.to('cuda:0'), labels.to('cuda:3')\n",
    "            outputs = net(inputs)\n",
    "            loss = criteria(outputs, labels)\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "            predict = torch.argmax(outputs, dim=1)\n",
    "            acc_sum += torch.sum(predict == labels).item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(\"Epoch: {}, Loss: {:.2f}, acc: {:.2f}, time cost: {:.2f}s\".format(epoch, loss_sum/len(train_loader), acc_sum/len(trainset), time.time()-t0))\n",
    "        \n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比一下相同GPU、相同环境下单卡运行时的log：\n",
    "\n",
    "```txt\n",
    "Cifar dataset already exist in './data', skip download\n",
    "Epoch: 0, Loss: 2.12, acc: 0.34, time cost: 15.32s\n",
    "Epoch: 1, Loss: 2.03, acc: 0.43, time cost: 14.14s\n",
    "Epoch: 2, Loss: 2.00, acc: 0.46, time cost: 14.19s\n",
    "Epoch: 3, Loss: 1.97, acc: 0.48, time cost: 14.51s\n",
    "Epoch: 4, Loss: 1.95, acc: 0.50, time cost: 14.23s\n",
    "Epoch: 5, Loss: 1.94, acc: 0.52, time cost: 14.22s\n",
    "Epoch: 6, Loss: 1.92, acc: 0.54, time cost: 14.27s\n",
    "Epoch: 7, Loss: 1.91, acc: 0.55, time cost: 14.32s\n",
    "Epoch: 8, Loss: 1.90, acc: 0.56, time cost: 13.78s\n",
    "Epoch: 9, Loss: 1.88, acc: 0.58, time cost: 14.36s\n",
    "```\n",
    "\n",
    "发现相较于单卡运行，简单地把模型拆成四块分到四张卡时，精度一致，节省了每张卡所消耗的显存，但没有起到特别明显的加速效果。究其原因，目前这种简单地将不同层放到不同GPU的做法显然效率低下.首先,GPU间复制数据的通信开销很大,在使用更大的数据做训练时这一点将更为明显.其次,顺序执行该模型的前向过程时,只有一张GPU在工作,其他的都在等待数据而白白浪费了资源.这一点就需要通过流水线并行来实现,会在第四部分详述."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于流水线并行，Pytorch tutorial还提供了一个简单的实现版本。我们对上面的`ConvNet()`类做一个封装，来实现流水线操作执行前向过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineConvNet(ConvNet):\n",
    "    \n",
    "    # 在父类的基础上，增加了一个split_size参数，用于指定流水线上的mini_batch_size\n",
    "    # 上面所指定的cifar数据集的batch_size为64，如果split_size=8，则每次计算8个数据，GPU0将这8个数据计算完毕并将结果传递给GPU1后，就可以立即计算下一批mini_batch了，而不需要一直等待\n",
    "    def __init__(self, split_size=8, *args, **kwargs):\n",
    "        super(PipelineConvNet, self).__init__(*args, **kwargs)\n",
    "        self.split_size = split_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        splits = x.split(self.split_size, dim=0)\n",
    "        s_next, s_prev_1, s_prev_2, s_prev_3 = None, None, None, None\n",
    "        ret = []\n",
    "        \n",
    "        # n卡做流水线，一共batch_size/split_size个mini_batch要处理，则需要batch_size/split_size+(n-1)次前向过程\n",
    "        assert len(splits) == batch_size // self.split_size\n",
    "        total_forward_len = len(splits) + (4 - 1)  # 在本次示例中，n=4，总共要执行的前向过程为64/8 + (4-1) = 11次\n",
    "        \n",
    "        for index in range(total_forward_len):\n",
    "            # 获取下一个要计算的mini-batch。注意：整个流水线的最后(n-1)个前向过程没有新的mini-batch被获取了\n",
    "            if index < len(splits):\n",
    "                s_next = splits[index]\n",
    "            else:\n",
    "                s_next = None\n",
    "                \n",
    "            # A. 在cuda:3上利用s_prev_3计算s_ret。这个顺序必须要是倒过来的，否则新的mini-batch的计算结果会覆盖掉上一个mini-batch的计算结果\n",
    "            if s_prev_3 is not None:\n",
    "                s_ret = self.out(s_prev_3.to('cuda:3'))\n",
    "                ret.append(s_ret)\n",
    "            \n",
    "            # B. 在cuda:2上利用s_prev_2计算s_prev_3\n",
    "            if s_prev_2 is not None:\n",
    "                s_prev_3 = self.fc(s_prev_2.to('cuda:2'))\n",
    "                \n",
    "            # C. 在cuda:1上利用s_prev_1计算s_prev_2\n",
    "            if s_prev_1 is not None:\n",
    "                s_prev_2 = self.conv2(s_prev_1.to('cuda:1'))\n",
    "            \n",
    "            # D. 在cuda:0上利用s_next计算s_prev_1\n",
    "            if s_next is not None:\n",
    "                s_prev_1 = self.conv1(s_next.to('cuda:0'))\n",
    "     \n",
    "        return torch.cat(ret, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这段代码其实挺冗长的，要实现的主要部分就是流水线式的前向过程。就像下图所示:\n",
    "\n",
    "![pipeline](../assets/pipeline.png)\n",
    "\n",
    "每个小方块就是一个mini-batch，会按流水线形式依次进行前向计算。后向计算的部分等后续再更新\n",
    "\n",
    "接下来简要用代码测试一下流水线的正确性（目前测不了高效性，hhh）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifar dataset already exist in '../2-DataParallel/data', skip download\n",
      "Epoch: 0, Loss: 2.12, acc: 0.33, time cost: 24.38s\n",
      "Epoch: 1, Loss: 2.04, acc: 0.41, time cost: 25.27s\n",
      "Epoch: 2, Loss: 2.00, acc: 0.45, time cost: 24.86s\n",
      "Epoch: 3, Loss: 1.97, acc: 0.49, time cost: 27.58s\n",
      "Epoch: 4, Loss: 1.95, acc: 0.51, time cost: 22.80s\n",
      "Epoch: 5, Loss: 1.93, acc: 0.53, time cost: 26.39s\n",
      "Epoch: 6, Loss: 1.92, acc: 0.54, time cost: 27.05s\n",
      "Epoch: 7, Loss: 1.90, acc: 0.56, time cost: 28.28s\n",
      "Epoch: 8, Loss: 1.89, acc: 0.57, time cost: 27.12s\n",
      "Epoch: 9, Loss: 1.88, acc: 0.58, time cost: 25.86s\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    net = PipelineConvNet()\n",
    "\n",
    "    trainset, testset = get_dataset(\"../2-DataParallel/data\")\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    for epoch in range(10):\n",
    "        t0 = time.time()\n",
    "        net.train()\n",
    "        \n",
    "        loss_sum,acc_sum = 0,0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            \n",
    "            #! 这里，输入张量在cuda:0上，标签张量在cuda:3上\n",
    "            inputs, labels = inputs.to('cuda:0'), labels.to('cuda:3')\n",
    "            outputs = net(inputs)\n",
    "            loss = criteria(outputs, labels)\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "            predict = torch.argmax(outputs, dim=1)\n",
    "            acc_sum += torch.sum(predict == labels).item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(\"Epoch: {}, Loss: {:.2f}, acc: {:.2f}, time cost: {:.2f}s\".format(epoch, loss_sum/len(train_loader), acc_sum/len(trainset), time.time()-t0))\n",
    "        \n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理想的情况当然是利用已有的库高效实现流水线并行，这部分我们在第四部分再说吧~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
