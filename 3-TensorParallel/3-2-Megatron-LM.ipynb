{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Megatron-LM张量并行\n",
    "\n",
    "Megatron是由 NVIDIA 的应用深度学习研究团队开发的大型、强大的Transformer模型，主要针对大规模训练大型 transformer 语言模型的研究。其主要贡献时提出了将模型进行横向分割而进行张量并行的思想。\n",
    "\n",
    "关于Tensor parallelism的中文解读可以参考：\n",
    "\n",
    "英伟达中国：https://zhuanlan.zhihu.com/p/420908718\n",
    "\n",
    "知乎大佬对具体切分方法的图示+伪代码：https://zhuanlan.zhihu.com/p/366906920\n",
    "\n",
    "博客园罗西的思考：https://www.cnblogs.com/rossiXYZ/p/15840803.html\n",
    "\n",
    "这一节Notebook主要想介绍针对一个比较简单的模型，如何使用Megatron快速将其进行张量并行的部署"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，安装megatron库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/Megatron-LM.git\n",
    "!cd Megatron-LM\n",
    "%pip install -v -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试安装是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'megatron' from '/mnt/configblob/users/ruizhe/Megatron-LM/megatron/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import megatron\n",
    "\n",
    "print(megatron)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的代码部分引用了Megatron单元测试的代码：https://github.com/NVIDIA/Megatron-LM/tree/main/tests/unit_tests\n",
    "\n",
    "我们首先引入Megatron内做模型并行初始化的API："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import megatron.core.parallel_state as ps\n",
    "from megatron.core.tensor_parallel.data import broadcast_data\n",
    "\n",
    "\n",
    "# 这个Utils类的作用是定义了初始化分布式环境，初始化模型并行环境，销毁模型并行环境的函数\n",
    "class Utils:\n",
    "\n",
    "    world_size = torch.cuda.device_count()\n",
    "    # 这个地方需要使用torchrun来启动分布式环境，否则这里的rank不会在环境变量中被发现，于是就直接报错了\n",
    "    rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_distributed():\n",
    "        print(f'Initializing torch.distributed with rank: {Utils.rank}, world_size: {Utils.world_size}')\n",
    "        torch.cuda.set_device(Utils.rank % torch.cuda.device_count())\n",
    "        init_method = 'tcp://'\n",
    "        master_ip = os.getenv('MASTER_ADDR', 'localhost')\n",
    "        master_port = os.getenv('MASTER_PORT', '6000')\n",
    "        init_method += master_ip + ':' + master_port\n",
    "        torch.distributed.init_process_group(backend='nccl', world_size=Utils.world_size, rank=Utils.rank, init_method=init_method)\n",
    "        \n",
    "    @staticmethod\n",
    "    def destroy_model_parallel():\n",
    "        ps.destroy_model_parallel()\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    ''' initial_model_parallel: 初始化模型并行环境：\n",
    "    tensor_model_parallel_size: 指定张量并行级别\n",
    "    pipeline_model_parallel_size: 指定模型并行级别\n",
    "    virtual_pipeline_model_parallel_size: 指定虚拟模型并行级别\n",
    "    pipeline_model_parallel_split_rank: 指定模型并行切分的rank\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def initialize_model_parallel(tensor_model_parallel_size = 1, pipeline_model_parallel_size = 1, virtual_pipeline_model_parallel_size = None, pipeline_model_parallel_split_rank = None):\n",
    "        ps.destroy_model_parallel()\n",
    "        if not torch.distributed.is_initialized():\n",
    "            Utils.initialize_distributed()\n",
    "        ps.initialize_model_parallel(tensor_model_parallel_size, pipeline_model_parallel_size, virtual_pipeline_model_parallel_size, pipeline_model_parallel_split_rank)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以定义一些单元测试，比如这里的代码测试megatron的张量广播功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试广播：指定张量并行级别为2，模型并行级别为4的这样的一个分布式环境，然后制造一些数据，看看broadcast_data的效果\n",
    "def test_broadcast_data():\n",
    "    Utils.initialize_model_parallel(2,4)\n",
    "    input_data = {\n",
    "        0 : torch.ones((8,8)).cuda() * 0.0,\n",
    "        1 : torch.ones((8,8)).cuda() * 1.0,\n",
    "        2 : torch.ones((8,8)).cuda() * 2.0,\n",
    "        3 : torch.ones((8,8)).cuda() * 3.0,\n",
    "        4 : torch.ones((8,8)).cuda() * 4.0,\n",
    "        5 : torch.ones((8,8)).cuda() * 5.0,\n",
    "        6 : torch.ones((8,8)).cuda() * 6.0,\n",
    "        7 : torch.ones((8,8)).cuda() * 7.0\n",
    "        }\n",
    "    dtype = torch.float32\n",
    "    # broadcast_data：将rank=0的进程的数据广播到所有进程\n",
    "    actual_output = broadcast_data([0,1],input_data, dtype)\n",
    "    assert(torch.equal(actual_output[0], input_data[0]))\n",
    "    assert(torch.equal(actual_output[1], input_data[1]))\n",
    "    \n",
    "    if Utils.rank == 0:\n",
    "        print(\"Broadcast assertion passed\")\n",
    "    Utils.destroy_model_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import megatron.core.tensor_parallel.utils as util\n",
    "\n",
    "# 测试all_gather\n",
    "def test_gather_split_1d_tensor():\n",
    "    rank = Utils.rank\n",
    "    Utils.initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=4)\n",
    "    input_tensor = torch.ones((2,4)).cuda() * rank\n",
    "    actual_output_tensor = util.gather_split_1d_tensor(input_tensor)\n",
    "    if rank %2 == 0:\n",
    "        expected_output_tensor = torch.concat((input_tensor.flatten(), input_tensor.flatten() + 1))\n",
    "    else : \n",
    "        expected_output_tensor = torch.concat((input_tensor.flatten() - 1, input_tensor.flatten()))\n",
    "    assert(torch.equal(actual_output_tensor, expected_output_tensor))\n",
    "    Utils.destroy_model_parallel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启动多卡的单元测试仍需要使用torchrun多卡启动程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Initializing torch.distributed with rank: 3, world_size: 8\n",
      "Initializing torch.distributed with rank: 1, world_size: 8\n",
      "Initializing torch.distributed with rank: 5, world_size: 8\n",
      "Initializing torch.distributed with rank: 2, world_size: 8\n",
      "Initializing torch.distributed with rank: 7, world_size: 8\n",
      "Initializing torch.distributed with rank: 4, world_size: 8\n",
      "Initializing torch.distributed with rank: 0, world_size: 8\n",
      "Initializing torch.distributed with rank: 6, world_size: 8\n",
      "Broadcast assertion passed\n",
      "rank: 0, input_tensor: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], device='cuda:0'); output_tensor: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "All_Gather assertion passed\n",
      "rank: 1, input_tensor: tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], device='cuda:1'); output_tensor: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:1')\n",
      "rank: 3, input_tensor: tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]], device='cuda:3'); output_tensor: tensor([2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "       device='cuda:3')\n",
      "rank: 2, input_tensor: tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]], device='cuda:2'); output_tensor: tensor([2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "       device='cuda:2')\n",
      "rank: 5, input_tensor: tensor([[5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5.]], device='cuda:5'); output_tensor: tensor([4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5., 5., 5.],\n",
      "       device='cuda:5')\n",
      "rank: 4, input_tensor: tensor([[4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.]], device='cuda:4'); output_tensor: tensor([4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5., 5., 5.],\n",
      "       device='cuda:4')\n",
      "rank: 7, input_tensor: tensor([[7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.]], device='cuda:7'); output_tensor: tensor([6., 6., 6., 6., 6., 6., 6., 6., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
      "       device='cuda:7')\n",
      "rank: 6, input_tensor: tensor([[6., 6., 6., 6.],\n",
      "        [6., 6., 6., 6.]], device='cuda:6'); output_tensor: tensor([6., 6., 6., 6., 6., 6., 6., 6., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
      "       device='cuda:6')\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=8 test_megatron.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里从all_gather的结果看出，因为是设置了张量并行级别为2，流水线并行级别为4，因此同一批次的tensor从横向是被划分到两张卡上（张量并行），所以`test_gather_split_1d_tensor()`这个函数的all_gather收集是横向收集的，而不涉及后续的纵向流水线并行。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
