{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Megatron-LM张量并行\n",
    "\n",
    "Megatron是由 NVIDIA 的应用深度学习研究团队开发的大型、强大的Transformer模型，主要针对大规模训练大型 transformer 语言模型的研究。其主要贡献时提出了将模型进行横向分割而进行张量并行的思想。\n",
    "\n",
    "关于Tensor parallelism的中文解读可以参考：\n",
    "\n",
    "英伟达中国：https://zhuanlan.zhihu.com/p/420908718\n",
    "\n",
    "知乎大佬对具体切分方法的图示+伪代码：https://zhuanlan.zhihu.com/p/366906920\n",
    "\n",
    "博客园罗西的思考：https://www.cnblogs.com/rossiXYZ/p/15840803.html\n",
    "\n",
    "这一节Notebook主要想介绍针对一个比较简单的模型，如何使用Megatron快速将其进行张量并行的部署"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，安装megatron库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/Megatron-LM.git\n",
    "!cd Megatron-LM\n",
    "%pip install -v -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试安装是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'megatron' from '/mnt/configblob/users/ruizhe/Megatron-LM/megatron/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import megatron\n",
    "\n",
    "print(megatron)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的代码部分引用了Megatron单元测试的代码：https://github.com/NVIDIA/Megatron-LM/tree/main/tests/unit_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import megatron.core.parallel_state as ps\n",
    "from megatron.core.tensor_parallel.data import broadcast_data\n",
    "\n",
    "class Utils:\n",
    "\n",
    "    world_size = torch.cuda.device_count()\n",
    "    rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_distributed():\n",
    "        print(f'Initializing torch.distributed with rank: {Utils.rank}, world_size: {Utils.world_size}')\n",
    "        torch.cuda.set_device(Utils.rank % torch.cuda.device_count())\n",
    "        init_method = 'tcp://'\n",
    "        master_ip = os.getenv('MASTER_ADDR', 'localhost')\n",
    "        master_port = os.getenv('MASTER_PORT', '6000')\n",
    "        init_method += master_ip + ':' + master_port\n",
    "        torch.distributed.init_process_group(backend='nccl', world_size=Utils.world_size, rank=Utils.rank, init_method=init_method)\n",
    "        \n",
    "    @staticmethod\n",
    "    def destroy_model_parallel():\n",
    "        ps.destroy_model_parallel()\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_model_parallel(tensor_model_parallel_size = 1, pipeline_model_parallel_size = 1, virtual_pipeline_model_parallel_size = None, pipeline_model_parallel_split_rank = None):\n",
    "        ps.destroy_model_parallel()\n",
    "        if not torch.distributed.is_initialized():\n",
    "            Utils.initialize_distributed()\n",
    "        ps.initialize_model_parallel(tensor_model_parallel_size, pipeline_model_parallel_size, virtual_pipeline_model_parallel_size, pipeline_model_parallel_split_rank)\n",
    "        \n",
    "def test_broadcast_data():\n",
    "    Utils.initialize_model_parallel(2,4)\n",
    "    input_data = {\n",
    "        0 : torch.ones((8,8)).cuda() * 0.0,\n",
    "        1 : torch.ones((8,8)).cuda() * 1.0,\n",
    "        2 : torch.ones((8,8)).cuda() * 2.0,\n",
    "        3 : torch.ones((8,8)).cuda() * 3.0,\n",
    "        4 : torch.ones((8,8)).cuda() * 4.0,\n",
    "        5 : torch.ones((8,8)).cuda() * 5.0,\n",
    "        6 : torch.ones((8,8)).cuda() * 6.0,\n",
    "        7 : torch.ones((8,8)).cuda() * 7.0\n",
    "        }\n",
    "    dtype = torch.float32\n",
    "    actual_output = broadcast_data([0,1],input_data, dtype)\n",
    "    assert(torch.equal(actual_output[0], input_data[0]))\n",
    "    assert(torch.equal(actual_output[1], input_data[1]))\n",
    "    Utils.destroy_model_parallel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分仍需要使用多卡来启动程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Initializing torch.distributed with rank: 1, world_size: 8\n",
      "Initializing torch.distributed with rank: 3, world_size: 8\n",
      "Initializing torch.distributed with rank: 7, world_size: 8\n",
      "Initializing torch.distributed with rank: 4, world_size: 8\n",
      "Initializing torch.distributed with rank: 2, world_size: 8\n",
      "Initializing torch.distributed with rank: 5, world_size: 8\n",
      "Initializing torch.distributed with rank: 6, world_size: 8\n",
      "Initializing torch.distributed with rank: 0, world_size: 8\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=8 test_megatron_broadcast.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
